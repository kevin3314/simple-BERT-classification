{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_pretrained_bert.modeling import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT model をロード\n",
    "bert_model_dir = '/mnt/larch/share/bert/Japanese_models/Wikipedia/L-12_H-768_A-12_E-30_BPE'\n",
    "bert_model = BertModel.from_pretrained(bert_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 32006\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# config を確認\n",
    "bert_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', 'の', '、', '。', 'に', 'は'],\n",
       " ['加', '世帯', 'テスト', 'ホー', '羽', '##公', '##西', '出た', '##型', '##ラス'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "# BPE で形態素をサブワードにトークナイズするためのオブジェクト\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_dir, do_lower_case=False) # 濁点対策\n",
    "(list(tokenizer.vocab)[:10],\n",
    "list(tokenizer.vocab)[3000:3010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentences: List[str] = [\n",
    "    '７ 　 自然 と 人間 野山 や 川 ， 海 など ， さまざまな 場所 に 数 多く の 生物 が 生活 して い ます 。',\n",
    "    '寂聴 　 ’ ０４ 　 みちのく 青空 説法 夏 の 法話 ― 出家 した 翌年 の 桜 は びっくり する ほど きれでした',\n",
    "    '屋久島 の 酸性 雨 被害 など 報告 大阪 で 危機 管理 講座 学校 法人 加計 学園 、'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] ７ 自然 と 人間 野 ##山 や 川 ， 海 など ， さまざまな 場所 に 数 多く の 生物 が 生活 して い ます 。 [SEP]',\n",
       " '[CLS] 寂 ##聴 ’ ０４ みちの ##く 青空 説 ##法 夏 の 法 ##話 ― 出家 した 翌年 の 桜 は び ##っくり する ほど きれ ##で ##した [SEP]',\n",
       " '[CLS] 屋 ##久 ##島 の 酸性 雨 被害 など 報告 大阪 で 危機 管理 講座 学校 法人 加 ##計 学園 、 [SEP]']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens: List[List[str]] = []\n",
    "for sentence in input_sentences:\n",
    "    input_tokens.append(['[CLS]'] + tokenizer.tokenize(sentence) + ['[SEP]'])\n",
    "[' '.join(tokens) for tokens in input_tokens]  # トークナイズされた文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2 75 1140 12 652 1981 444 34 246 176 573 42 176 2884 463 8 145 135 5 1323 11 580 19 142 1953 7 3',\n",
       " '2 17394 11165 699 10632 26592 712 24852 791 1643 835 5 202 5143 6086 9746 20 1044 5 2847 9 5305 19537 22 500 7860 429 1033 3',\n",
       " '2 1552 2191 760 5 17609 2899 1412 42 1036 340 13 2783 567 7015 172 907 3001 12845 1769 6 3']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids: List[List[int]] = []\n",
    "for tokens in input_tokens:\n",
    "    input_ids.append(tokenizer.convert_tokens_to_ids(tokens))\n",
    "[' '.join(str(id_) for id_ in ids) for ids in input_ids]  # トークナイズ&ID化された文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment_id(token_type_ids) と input_mask(attention_mask) を作成\n",
    "max_seq_len: int = max(len(ids) for ids in input_ids)  # トークン列の最大長\n",
    "segment_ids: List[List[int]] = []\n",
    "input_mask: List[List[int]] = []\n",
    "for idx, ids in enumerate(input_ids):\n",
    "    seq_len = len(ids)\n",
    "    pad: List[int] = [0] * (max_seq_len - seq_len)  # パディング\n",
    "    input_ids[idx] += pad\n",
    "    segment_ids.append([0] * max_seq_len) # 文対を扱うタスクではないので全てゼロ\n",
    "    input_mask.append([1] * seq_len + pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2 75 1140 12 652 1981 444 34 246 176 573 42 176 2884 463 8 145 135 5 1323 11 580 19 142 1953 7 3 0 0',\n",
       " '0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0',\n",
       " '1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最初の文に対して input_ids, segment_ids, input_mask を表示\n",
    "(' '.join(str(x) for x in input_ids[0]),\n",
    "' '.join(str(x) for x in segment_ids[0]),\n",
    "' '.join(str(x) for x in input_mask[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パディングできたので torch.Tensor に変換\n",
    "input_ids = torch.tensor(input_ids)      # (3, 27)\n",
    "segment_ids = torch.tensor(segment_ids)  # (3, 27)\n",
    "input_mask = torch.tensor(input_mask)    # (3, 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUに送る\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # 各自、空いている gpu_id に書き換え\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "bert_model.to(device)\n",
    "input_ids = input_ids.to(device)\n",
    "segment_ids = segment_ids.to(device)\n",
    "input_mask = input_mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 29, 768]), torch.Size([3, 768]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward\n",
    "encoded_layers, pooled_output = bert_model(input_ids, \n",
    "                                           token_type_ids=segment_ids,\n",
    "                                           attention_mask=input_mask,\n",
    "                                           output_all_encoded_layers=False)\n",
    "(encoded_layers.size(), pooled_output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "additional_layer = nn.Linear(768, 4).to(device)  # 4クラス分類の時\n",
    "output = additional_layer(pooled_output)\n",
    "output.size()\n",
    "# このあと、cross entropy loss をとったり、back propagation したり"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
